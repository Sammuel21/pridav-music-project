{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling schema\n",
    "\n",
    "This notebook provides schema for preparing data transformation pipeline for modelling, data needs to be transformed to model ready form.  \n",
    "This notebook serves as a guide, feel free to modify your code however you want - it just serves to explain the transformation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "import etl\n",
    "\n",
    "importlib.reload(etl)\n",
    "from etl import (\n",
    "    FrequencyEncoder,\n",
    "    CircleOfFifthsEncoding,\n",
    "    ConvertNull,\n",
    "    ArtistPopularityEncoder,\n",
    "    FollowerCountEncoder,\n",
    "    AlbumNameEncoder,\n",
    "    GenreEncoder,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './Data/'\n",
    "TRACK_FILE = 'spotify_tracks_kaggle_weekly.csv'\n",
    "ARTIST_FILE = 'spotify_tracks_artist_details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv(DATA_DIR + TRACK_FILE)\n",
    "artists = pd.read_csv(DATA_DIR + ARTIST_FILE)\n",
    "\n",
    "data = pd.merge(tracks, artists, on='track_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting -> working with predefined $X_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 21\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('popularity', axis=1)\n",
    "y = data['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns defined in EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['track_id', 'artwork_url', 'track_url', 'track_name_x', 'track_name_y', 'artist_ids', 'artist_names']\n",
    "\n",
    "# removed the calculation from scraper as it is added in etl.py\n",
    "# this is in case data is not re-run\n",
    "if 'avg_artist_popularity' in X_train.columns:\n",
    "    drop_columns.append('avg_artist_popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(drop_columns, axis=1, errors='ignore')\n",
    "X_test = X_test.drop(drop_columns, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'popularity'\n",
    "\n",
    "onehot_col = ['language']\n",
    "circle_of_fifths_col = ['key']\n",
    "artist_name_col = ['artist_name']\n",
    "album_name_col = ['album_name']\n",
    "artist_popularity_col = ['artist_popularities']\n",
    "follower_count_col = ['artist_followers']\n",
    "artist_genres_col = ['artist_genres']\n",
    "\n",
    "numeric_columns = list(X_train.columns[X_train.dtypes != object].difference(['key', 'mode', 'artist_popularities', 'artist_followers']))\n",
    "\n",
    "nan_columns = ['acousticness', 'danceability', 'energy', 'liveness', 'speechiness', 'tempo', 'valence', 'artist_popularities', 'artist_followers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "artist_name_pipeline = Pipeline(steps=[\n",
    "    ('encoding', FrequencyEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "album_name_pipeline = Pipeline(steps=[\n",
    "    ('encoding', AlbumNameEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "artist_popularity_pipeline = Pipeline(steps=[\n",
    "    ('encoding', ArtistPopularityEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "artist_followers_pipeline = Pipeline(steps=[\n",
    "    ('encoding', FollowerCountEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "artist_genres_pipeline = Pipeline(steps=[\n",
    "    ('encoding', GenreEncoder()),\n",
    "    ('scaling', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "transformations = ColumnTransformer(transformers=[\n",
    "    \n",
    "    ('onehot_encoding', OneHotEncoder(sparse_output=False), onehot_col),\n",
    "    ('trigonometric_encoding', CircleOfFifthsEncoding(), circle_of_fifths_col),\n",
    "    ('artist_encoding', artist_name_pipeline, artist_name_col),\n",
    "    ('album_encoding', album_name_pipeline, album_name_col),\n",
    "    ('artist_popularity_encoding', artist_popularity_pipeline, artist_popularity_col),\n",
    "    ('follower_count_encoding', artist_followers_pipeline, follower_count_col),\n",
    "    ('genres_encoding', GenreEncoder(), artist_genres_col),\n",
    "    ('nummeric_processing', numeric_pipeline, numeric_columns)\n",
    "\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('null_values', ConvertNull(columns=nan_columns)),\n",
    "    ('transformation', transformations)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessing.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "RF_param_grid = {\n",
    "    'model__n_estimators': [250, 300],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [8, 10],\n",
    "    'model__min_samples_leaf': [3, 5]\n",
    "}\n",
    "\n",
    "RF_grid_search = GridSearchCV(estimator=RF_model, param_grid=RF_param_grid, \n",
    "                              cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "RF_grid_search.fit(X_train, y_train)\n",
    "\n",
    "RF_best_model = RF_grid_search.best_estimator_\n",
    "RF_best_params = RF_grid_search.best_params_\n",
    "print(\"Best Parameters:\", RF_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF_best_model.predict(X_test)\n",
    "\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('R2: ', RF_best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Results\n",
    "RF_param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [5, 10],\n",
    "    'model__min_samples_leaf': [2, 4]\n",
    "}\n",
    "best: 200, None, 10, 4\n",
    "    \n",
    "RF_param_grid = {\n",
    "    'model__n_estimators': [200, 250],\n",
    "    'model__max_depth': [None, 2],\n",
    "    'model__min_samples_split': [10, 12],\n",
    "    'model__min_samples_leaf': [4, 6]\n",
    "}\n",
    "best: 250, None, 10, 4\n",
    "R2:  0.5890150062574195\n",
    "\n",
    "RF_param_grid = {\n",
    "    'model__n_estimators': [250, 300],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [8, 10],\n",
    "    'model__min_samples_leaf': [3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', XGBRegressor(objective='reg:squarederror', random_state=RANDOM_STATE, \n",
    "                           n_estimators=127, max_depth=12, learning_rate=0.05, min_child_weight=7))\n",
    "])\n",
    "\n",
    "XGB_model.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [127, 130, 140],\n",
    "    'model__max_depth': [12],\n",
    "    'model__learning_rate': [0.04, 0.05],\n",
    "    'model__min_child_weight': [6, 7]\n",
    "}\n",
    "\n",
    "XGB_grid_search = GridSearchCV(\n",
    "    estimator=XGB_model,\n",
    "    param_grid=XGB_param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "XGB_grid_search.fit(X_train, y_train)\n",
    "\n",
    "XGB_best_model = XGB_grid_search.best_estimator_\n",
    "XGB_best_params = XGB_grid_search.best_params_\n",
    "print(\"Best Parameters:\", XGB_best_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = XGB_model.predict(X_test)\n",
    "\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred))\n",
    "print('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('R2: ', XGB_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Results\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [150, 200, 250],\n",
    "    'model__max_depth': [6, 9, 12],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__min_child_weight': [1, 3]\n",
    "}\n",
    "best: 150, 12, 0.05, 3\n",
    "R2: 0.5993564128875732\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [100,150, 175],\n",
    "    'model__max_depth': [12, 15, 18],\n",
    "    'model__learning_rate': [0.25, 0.75],\n",
    "    'model__min_child_weight': [3, 5]\n",
    "}\n",
    "best: 100, 12, 0.25, 5\n",
    "R2:  0.5655938982963562\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [75, 100, 150],\n",
    "    'model__max_depth': [9, 12, 15],\n",
    "    'model__learning_rate': [0.03, 0.05],\n",
    "    'model__min_child_weight': [5, 7]\n",
    "}\n",
    "best: 150, 12, 0.05, 7\n",
    "R2:  0.6038635969161987\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [125, 150, 175],\n",
    "    'model__max_depth': [11, 12, 13],\n",
    "    'model__learning_rate': [0.05, 0.07],\n",
    "    'model__min_child_weight': [7, 9]\n",
    "}\n",
    "best: 125, 12, 0.05, 7\n",
    "R2:  0.605049729347229\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [120, 125, 130],\n",
    "    'model__max_depth': [12],\n",
    "    'model__learning_rate': [0.04, 0.06],\n",
    "    'model__min_child_weight': [6, 8]\n",
    "}\n",
    "best: 130, 12, 0.04, 6\n",
    "R2:  0.6025806069374084\n",
    "\n",
    "XGB_param_grid = {\n",
    "    'model__n_estimators': [127, 130, 140],\n",
    "    'model__max_depth': [12],\n",
    "    'model__learning_rate': [0.04, 0.05],\n",
    "    'model__min_child_weight': [6, 7]\n",
    "}\n",
    "best: 127, 12, 0.05, 7\n",
    "R2:  0.6050568222999573"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
